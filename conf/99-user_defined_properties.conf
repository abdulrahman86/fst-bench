# Hibench properties
hibench.hadoop.home		/home/lv/intel/hadoop/hadoop-1.2.1
#hibench.hadoop.home		/home/lv/intel/hadoop/hadoop-2.5.2
#hibench.hadoop.home		/home/lv/intel/hadoop/cdh4/hadoop-2.0.0-cdh4.7.1
#hibench.hadoop.executable	${hibench.hadoop.home}/bin-mapreduce1/hadoop
#hibench.hadoop.configure.dir	${hibench.hadoop.home}/etc/hadoop-mapreduce1
#hibench.hadoop.home		/home/lv/intel/hadoop/cdh5/hadoop-2.5.0-cdh5.3.2
#hibench.spark.home		/home/lv/intel/spark/dist-1.2.2-hive-hadoop1.0.4
#hibench.spark.home		/home/lv/intel/spark/dist-1.2.2-hive-yarn-hadoop2.4
#hibench.spark.home		/home/lv/intel/spark/dist-1.2.2-hive-hadoop2.4
hibench.spark.home		/home/lv/intel/spark/dist
hibench.hdfs.master		hdfs://localhost:54310

# spark master: spark://xxx:7077 => standalone mode, yarn-client => yarn mode
hibench.spark.master		spark://pxe64:7077
#hibench.spark.master		yarn-client

hibench.default.map.parallelism		2
hibench.default.shuffle.parallelism	2

# executor memory
spark.executor.memory  7G

# Compression
spark.rdd.compress            false
# compression codec: lz4, lzf, snappy, put class path here accordingly.
spark.io.compression.codec    org.apache.spark.io.SnappyCompressionCodec 

# Akka
spark.akka.frameSize          1000
spark.akka.timeout            600

# mllib will use KyroSerializer, ensure the buffer is large enough
spark.kryoserializer.buffer.mb	 2000

# Scale profile: tiny, small, large, ..., defined in 20-data-scale-profile.conf
hibench.scale.profile  	      	     	tiny
# Compression options selection: enable, disable
hibench.compress.profile	 	enable
# Compression codec profile selection:	 snappy, lzo, default
hibench.compress.codec.profile		snappy
