#======================================================
# Mandatory settings
#======================================================

# Hadoop home
#hibench.hadoop.home		/home/lv/intel/hadoop/hadoop-1.2.1
#hibench.hadoop.home		/home/lv/intel/hadoop/hadoop-2.4.1
#hibench.hadoop.home		/home/lv/intel/hadoop/cdh4/hadoop-2.0.0-cdh4.7.1
hibench.hadoop.home		/home/lv/intel/hadoop/cdh5/hadoop-2.5.0-cdh5.3.2
#hibench.hadoop.home		/home/lv/project/cluster/hadoop-2.5.0-cdh5.3.2

# Spark home
#hibench.spark.home		/home/lv/intel/spark/dist-1.2.2-hive-hadoop1.0.4
#hibench.spark.home		/home/lv/intel/spark/dist-1.2.2-hive-yarn-hadoop2.4
#hibench.spark.home		/home/lv/intel/cluster/spark/dist-spark1.2-hadoop2.4
hibench.spark.home		/home/lv/intel/cluster/spark/spark-1.3.0-bin-hadoop2.4
#hibench.spark.home		/home/lv/intel/cluster/spark/spark-1.2.1-bin-hadoop2.4

# HDFS master
#hibench.hdfs.master		hdfs://homeserver:54310
hibench.hdfs.master		hdfs://lv-dev:54310

# Spark master
#   standalone mode: `spark://xxx:7077`
#   YARN mode: `yarn-client`
#   unset: fallback to `local[1]`
hibench.spark.master		spark://lv-dev:7077
#hibench.spark.master		yarn-client
#hibench.spark.master		yarn-cluster

#======================================================
# Not mandatory but important settings
#======================================================

# `hibench.hadoop.executable` is used to auto probe hadoop version and
# hadoop release, which is critical for further configurations. Most
# cases `hadoop` executable be placed under HADOOP_HOME/bin. However,
# in some cases such as CDH5/MR1, it must be explicitly defined:

hibench.hadoop.executable	${hibench.hadoop.home}/bin-mapreduce1/hadoop

# `hibench.spark.version` is used to choose which sparkbench workload
# jar. Mostly situation it'll be auto probed. Please override if spark
# version is not probed correctly. 
# Note, supported values: `spark1.2` or `spark1.3`

#hibench.spark.version          spark1.3

#======================================================
# Optional settings
#======================================================

# Important parameters
#---------------------

# execute parallelism settings
hibench.default.map.parallelism		2
hibench.default.shuffle.parallelism	2

# YARN resource configuration
hibench.yarn.exector.num	1
hibench.yarn.exector.cores	1

# Spark only properties
#----------------------

# executor/driver memory in standalone & YARN mode
spark.executor.memory  1G
spark.driver.memory    1G

# Compression
spark.rdd.compress            false
# compression codec: lz4, lzf, snappy, put class path here accordingly.
spark.io.compression.codec    org.apache.spark.io.SnappyCompressionCodec 

# Akka
spark.akka.frameSize          1000
spark.akka.timeout            600

# mllib will use KyroSerializer, ensure the buffer is large enough
spark.kryoserializer.buffer.mb	 2000

# Data scale, Compression profile selection
#------------------------------------------

# Data scale profile: tiny, small, large, ..., defined in 10-data-scale-profile.conf
hibench.scale.profile  	      	     	tiny
# Compression options selection: enable, disable
hibench.compress.profile	 	disable	
# Compression codec profile selection:	 snappy, lzo, default
hibench.compress.codec.profile		snappy
