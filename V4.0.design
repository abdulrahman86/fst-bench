Requirements
============

# verification
1. configuration self verification & environment check: check to ensure each node is prepared for benchmarking.

# function enhancement
2. concurrent mode: execute several workload one time
3. workload migration for hadoop & spark: hadoop, spark, what ever. Just a blackbox.
4. share datasets: for large scale benchmark, dataset will consume significant disk space and generation time
5. dataset dump & restore: sometime dataset generation is too time consuming, rather backup and restore it from local disk.

# automation
5. configuration migration: only one place for tuning.
6. regression test utilites: run-all, reset-all

# output
7. log collection: ulimit, env, workload config, hadoop parameters, spark parameters, all logs in worker, master

Design
======
1. Remove env based configuration like hibench-config.sh, sparkbench-config.sh, etc
2. All configurations goes into one file, or one folder, which can be backup / restore / replace easily.
3. All configuration parameters must be explicitly defined. 
4. all configuration goes into conf/ folder, leave all workloads a bin folder with a run.sh only
   

Modules
=======
1. parser for conf of hadoop1&2, spark 
2. utilites to manage node sync, privileges check, folder clean, ... 
3. entrance for workload execution
4. code merge for hibench's autogen and sparkbench