#bin directory of spark installation (will use spark-submit)
#SPARK_BIN_DIR=/home/lv/intel/cluster/spark/spark-1.3.0-bin-hadoop2.4/bin
####### comment: redirect to ${SPARK_HOME}/bin

# prepare config

#Please ensure the topic is created in kafka
# the kafka topic under which new data are generated
#topic=
####### comment: map to topicName


# Two data sets(text and numeric) are available, app argument indicates to use which
#app=micro/sketch      #use text dataset, avg record size: 60 bytes
#app=micro/statistics #use numeric dataset, avg record size: 200 bytes
hibench.streamingbench.app        micro/sketch
#hibench.streamingbench.prepare.app        micro/statistics

# Kafka brokers that data will be generated to
#kafkabrokers=
####### comment: map to broker_list_with_quote

# Text dataset can be scaled in terms of record size
#textdataset_recordsize_factor=
hibench.streamingbench.prepare.textdataset_recordsize_factor 

# Two modes of generator: push,periodic
# Push means to send data to kafka cluster as fast as it could
# Periodic means sending data according to sending rate specification
#mode=
hibench.streamingbench.prepare.mode     push
#hibench.streamingbench.prepare.mode     periodic

# Under push mode: number of total records that will be generated
#records=
hibench.streamingbench.prepare.push.records	900000000

# Following three params are under periodic mode
# Record count per interval
#recordPerInterval=
hibench.streamingbench.prepare.periodic.recordPerInterval

# Interval time (in ms)
#intervalSpan=
hibench.streamingbench.prepare.periodic.intervalSpan

# Total round count of data send
#totalRound=
hibench.streamingbench.prepare.periodic.totalRound


###############

#Cluster config
# Spark master location
#sparkMaster=spark://lv-dev:7077
####### comment: redirect to SPARK_MASTER

# zookeeper host of kafka cluster
#zkHost=localhost
####### comment: add
hibench.streamingbench.zookeeper.host	lv-dev

###############

#Parallel config
# number of nodes that will receive kafka input
#receiverNodes=48
####### comment: add
hibench.streamingbench.receiver_nodes	48

###############
#Benchmark args
#Note to ensure benchName to be consistent with datagen type. Numeric data for statistics and text data for others
# please uncomment one benchName to run the benchmark
#benchName="micro/identity"
#benchName="micro/sample"
#benchName="micro/sketch"
#benchName="micro/grep"
#benchName="micro/wordcount"
#benchName="micro/distinctcount"
#benchName="micro/statistics"
####### comment: add
# available benchname: micro-identity micro-sample micro-sketch micro-grep micro-wordcount micro-distinctcount micro-statistics
# trident-identity trident-sample trident-sketch trident-grep trident-wordcount trident-distinctcount trident-statistics

hibench.streamingbench.benchname	micro-identity
#hibench.streamingbench.benchname	trident-identity

#common args
# the topic that spark will receive input data
#topicName=identity-source-60
# commont: add
hibench.streamingbench.topic_name	identity-source-60

# Spark stream batch interval
#batchInterval=50  #In seconds
####### comment: add
hibench.streamingbench.batch_interval	50

# consumer group of the spark consumer for kafka
# consumerGroup=xxx
# duplicated with "consumer"
hibench.streamingbench.consumer_group	xxx

# expected number of records to be processed
#recordCount=900000000
hibench.streamingbench.record_count	900000000

#sketch/distinctcount/statistics arg
# the field index of the record that will be extracted
#fieldIndex=1
hibench.streamingbench.field_index	1

#sketch/wordcount/distinctcount/statistics arg
# the seperator between fields of a single record
#separator=\\s+
hibench.streamingbench.separator	\\s+

#sample arg
# probability that a record will be taken as a sample
#prob=0.1
hibench.streamingbench.prob		0.1

#grep arg
# the substring that will be checked to see if contained in a record
#pattern=the
hibench.streamingbench.pattern		the

#common arg
# indicate RDD storage level. 
# 1 for memory only 1 copy. Others for default mem_disk_ser 2 copies 
#copies=2
hibench.streamingbench.copies		2

# indicate whether to test the write ahead log new feature
# set true to test WAL feature
#testWAL=false
hibench.streamingbench.testWAL		false

# if testWAL is true, this path to store stream context in hdfs shall be specified. If false, it can be empty
#checkpointPath=
hibench.streamingbench.checkpoint_path	

#common arg
# indicate whether in debug mode for correctness verfication
#debug=false
hibench.streamingbench.debug		false

# whether to use direct approach or not
#directMode=true
hibench.streamingbench.direct_mode	true

# Kafka broker lists, used for direct mode, written in mode "host:port,host:port,..."
#brokerList=""
hibench.streamingbench.brokerList	lv-dev:9092
hibench.streamingbench.broker_list_with_quote	"${hibench.streamingbench.brokerList}"




# storm bench conf

#STORM_BIN_HOME=?
####### comment: add
hibench.streamingbench.storm.home	/home/lv/intel/cluster/storm/0.9.3/apache-storm-0.9.3
hibench.streamingbench.storm.bin	${hibench.streamingbench.storm.home}/bin


###############

#Cluster config
# nimbus of storm cluster
#nimbus=
#nimbusAPIPort=6627
####### comment: add
hibench.streamingbench.storm.nimbus		lv-dev
hibench.streamingbench.storm.nimbusAPIPort	6627

# time interval to contact nimbus to judge if finished
#nimbusContactInterval=
hibench.streamingbench.storm.nimbusContactInterval	10


###############

#Parallel config
# number of workers of Storm. Number of most bolt threads is also equal to this param.
#workerCount=
hibench.streamingbench.storm.worker_count	12

# number of kafka spout threads of Storm
#spoutThreads=
hibench.streamingbench.storm.spout_threads	12

# number of bolt threads altogether
#boltThreads=
hibench.streamingbench.storm.bolt_threads	12

# kafka arg indicating whether to read data from kafka from the start or go on to read from last position
#readFromStart=false
hibench.streamingbench.storm.read_from_start	false

# whether to turn on ack
#ackon=true
hibench.streamingbench.storm.ackon		true

###############

#Benchmark args
#Note to ensure benchName to be consistent with datagen type. Numeric data for statistics and text data for others
# please  uncomment one benchName to run the benchmark
#benchName=micro-identity
#benchName=micro-sample
#benchName=micro-sketch
#benchName=micro-grep
#benchName=micro-wordcount
#benchName=micro-distinctcount
#benchName=micro-statisticssep
#common args
# the topic that storm will receive input data
#topic=
# target number of records to be processed
#recordCount=
####### comments: follow sparkstreaming's conf

# consumer group of the storm consumer for kafka
#consumer=
####### what's the differenct with sparkstreaming's consumerGroup?
####### No difference!
#hibench.streamingbench.storm.consume

#sketch/wordcount/distinctcount/statistics arg
# the seperator between fields of a single record 
#separator=\\s+

#sketch/distinctcount/statistics arg
# the field index of the record that will be extracted
#fieldIndex=1

#sample arg
# probability that a record will be taken as a sample 
#prob=0.1

#grep arg
# the substring that will be checked to see if contained in a record
#pattern=the



# added by Lv:
hibench.streamingbench.jars		${hibench.streamingbench.sparkbench.jar}
hibench.streamingbench.sparkbench.jar	${hibench.home}/src/streambench/sparkbench/target/streaming-bench-spark_0.1-4.0-SNAPSHOT-jar-with-dependencies.jar
hibench.streamingbench.stormbench.jar	${hibench.home}/src/streambench/stormbench/target/streaming-bench-storm-0.1-SNAPSHOT-jar-with-dependencies.jar


