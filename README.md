# HiBench Suite #
## The bigdata micro benchmark suite ##


- Current version: 5.0
- Release date: 2015-10-12
- Contact: [Lv Qi](mailto:qi.lv@intel.com), [Grace Huang](mailto:jie.huang@intel.com), [Jiangang Duan] (mailto:jiangang.duan@intel.com)
- Homepage: https://github.com/intel-hadoop/HiBench

- Contents:
  1. Overview
  2. Getting Started
  3. Advanced Configuration
  4. Possible Issues

---
### OVERVIEW ###

This benchmark suite contains 10 typical micro workloads. This benchmark suite also has options for users to enable input/output compression for most workloads with default compression codec (zlib). Some initial work based on this benchmark suite please refer to the included ICDE workshop paper (i.e., WISS10_conf_full_011.pdf).

Note: 
 1. Since HiBench-2.2, the input data of benchmarks are all automatically generated by their corresponding prepare scripts.
 2. Since HiBench-3.0, it introduces Yarn support
 3. Since HiBench-4.0, it consists of more workload implementations on both Hadoop MR and Spark. For Spark, three different APIs including Scala, Java, Python are supportive.
 4. Since HiBench-5.0, it introduces Streaming related workloads including 4 frameworks: SparkStreaming, Storm, Storm-Trident and Samza.

  **Job based Micro benchmarks:**

  For job based tasks (in the contrast to streaming based tasks), HiBench provide following workloads:

1. Sort (sort)

    This workload sorts its *text* input data, which is generated using RandomTextWriter.

2. WordCount (wordcount)

    This workload counts the occurrence of each word in the input data, which are generated using RandomTextWriter. It is representative of another typical class of real world MapReduce jobs - extracting a small amount of interesting data from large data set.
	
3. TeraSort (terasort)

    TeraSort is a standard benchmark created by Jim Gray. Its input data is generated by Hadoop TeraGen example program.

4. Sleep (sleep)

    This workload sleep an amount of seconds in each task to test framework scheduler.

  **SQL:**

5. Scan (scan), Join(join), Aggregate(aggregation)

    This workload is developed based on SIGMOD 09 paper "A Comparison of Approaches to Large-Scale Data Analysis" and HIVE-396. It contains Hive queries (Aggregation and Join) performing the typical OLAP queries described in the paper. Its input is also automatically generated Web data with hyperlinks following the Zipfian distribution.

  **Web Search Benchmarks:**

6. PageRank (pagerank)

    This workload benchmarks PageRank algorithm implemented in Spark-MLLib/Hadoop (a search engine ranking benchmark included in pegasus 2.0) examples. The data source is generated from Web data whose hyperlinks follow the Zipfian distribution.
	
7. Nutch indexing (nutchindexing)

    Large-scale search indexing is one of the most significant uses of MapReduce. This workload tests the indexing sub-system in Nutch, a popular open source (Apache project) search engine. The workload uses the automatically generated Web data whose hyperlinks and words both follow the Zipfian distribution with corresponding parameters. The dict used to generate the Web page texts is the default linux dict file /usr/share/dict/linux.words.

  **Machine Learning:**

9. Bayesian Classification (bayes)

    This workload benchmarks NaiveBayesian Classification implemented in Spark-MLLib/Mahout examples.

    Large-scale machine learning is another important use of MapReduce. This workload tests the Naive Bayesian (a popular classification algorithm for knowledge discovery and data mining)  trainer in Mahout 0.7, which is an open source (Apache project) machine learning library. The workload uses the automatically generated documents whose words follow the zipfian distribution. The dict used for text generation is also from the default linux file /usr/share/dict/linux.words.

10. K-means clustering (kmeans)

    This workload tests the K-means (a well-known clustering algorithm for knowledge discovery and data mining) clustering in Mahout 0.7/Spark-MLlib. The input data set is generated by GenKMeansDataset based on Uniform Distribution and Guassian Distribution.

  **HDFS Benchmarks:**

11. enhanced DFSIO (dfsioe)

    Enhanced DFSIO tests the HDFS throughput of the Hadoop cluster by generating a large number of tasks performing writes and reads simultaneously. It measures the average I/O rate of each map task, the average throughput of each map task, and the aggregated throughput of HDFS cluster. Note: this benchmark doesn't have Spark corresponding implementation.

  **Streaming based Micro benchmarks:**

12. Streaming (streamingbench)

  Starting from HiBench 5.0, we provide following streaming workloads for SparkStreaming, Storm, Storm-Trident and Samza:

  Benchmark | Data type | Complexity | Store state involvment
  ----------|-----------|------------|-----------------------
  Identity  | Text      | Single Step| Not Involved
  Sample    | Text      | Single Step| Not Involved
  Project   | Text      | Single Step| Not Involved
  Grep      | Text      | Single Step| Not Involved
  Wordcount | Text      | Multi Step |  Involved
  Distinctcount| Text   | Multi Step |  Involved
  Statistics| Numeric   | Multi Step |  Involved

  a) Data type
  
  Big data benchmarks can be roughly classified into two types, Textual and Numeric. Text data is converted from the data source in SQL related benchmarks, which is user visit logs generated by Zipfian  distribution. Numeric data is converted from vectors in Kmeans data samples.

  b) Complexity

  Some basic opertions are essential for understanding in any data stream computation frameworks, including identity, sample, project and grep. And multi-step operations like wordcount, distinctcount and statistics are considered for sophisticated applications.
  
  c) Store state

  One feature of stream processing is integrating stored and streaming data, which may require referring to historical information and may result in updating global status either in disk or in memory. wordcount, distinctcount and statistics are provied for a demonstration for such reqirement.
    
**Supported hadoop/spark/storm/samza release:**

  - Apache release of Hadoop 1.x and Hadoop 2.x
  - CDH4/CDH5 release of MR1 and MR2.
  - Spark1.2 - 1.5
  - Storm 0.9.3
  - Samza 0.8.0

---
### Getting Started Without StreamingBench ###

1. System setup.

     Setup JDK, Hadoop-YARN, Spark runtime environment properly.

     Download/checkout HiBench benchmark suite

     Run `<HiBench_Root>/bin/build-all.sh` to build HiBench.
      
     Note: Begin from HiBench V4.0, HiBench will need python 2.x(>=2.6) .

2. [HiBench Configurations.](#hibenchconf)

     For minimum requirements: create & edit `conf/99-user_defined_properties.conf`ï¼š
     
          cd conf 
          cp 99-user_defined_properties.conf.template 99-user_defined_properties.conf
     
     And Make sure below properties has been set:

          hibench.hadoop.home      The Hadoop installation location
          hibench.spark.home       The Spark installation location
          hibench.hdfs.master      HDFS master
          hibench.spark.master     SPARK master
	  
     Note: For YARN mode, set `hibench.spark.master` to `yarn-client`. (`yarn-cluster` is not supported yet)

3. Run

   Execute the `<HiBench_Root>/bin/run-all.sh` to run all workloads with all language APIs with `large` data scale.

4. View the report:
   
   Goto `<HiBench_Root>/report` to check for the final report:
      - `report/hibench.report`: Overall report about all workloads.
      - `report/<workload>/<language APIs>/bench.log`: Raw logs on client side.
      - `report/<workload>/<language APIs>/monitor.html`: System utilization monitor results.
      - `report/<workload>/<language APIs>/conf/<workload>.conf`: Generated environment variable configurations for this workload.
      - `report/<workload>/<language APIs>/conf/sparkbench/<workload>/sparkbench.conf`: Generated configuration for this workloads, which is used for mapping to environment variable.
      - `report/<workload>/<language APIs>/conf/sparkbench/<workload>/spark.conf`: Generated configuration for spark.
      
   [Optional] Execute `<HiBench root>/bin/report_gen_plot.py report/hibench.report` to generate report figures.

   Note: `report_gen_plot.py` requires `python2.x` and `python-matplotlib`.

---
### Getting Started for StreamingBench ###

1. Prerequirements.

     Finish configurations described in above section. Hadoop cluster and spark must be properly configured. For running Samza, Hadoop YARN cluster is needed.

     Download & setup ZooKeeper (3.3.3 is preferred).

     Download & setup Apache Kafka (0.8.1, scala version 2.10 is preferred).

     Download & setup Apache Storm (0.9.3 is preferred).

2. ZooKeeper setup

     Edit the config file in zookeeper installation directory, please refer to conf/example/zookeeper for an example.

     Go to the install directory of Zookeeper, start Zookeeper with that config file.

     You may run `bin/zkCli.sh` to verify if zookeeper is working properly.

     Sometimes you may need to clean up the data inside zookeeper. First stop the server, then run "rm -rf /path/to/zookeeper/datadir" to clean the data dir. The directory is defined in your config file.
     
2. Kafka setup

     When configuring Kafka and topic count, we need to ensure disk won't become bottleneck. It is suggested to start several brokers in each kafka node, and configure each broker several disks. Different brokers in the same node may share disks but have their own directories in the same disk. Our topic count is 16 for each kafka node, that is, if the kafka cluster contains only 1 kafka node, then we create topics with 16 partitions. For environment with 3 kafka nodes, we create topics with 48 partitions.

     A typical set of kafka config files are config/serv1.properties till serv4.properties under kafka installation directory.

     Ensure zookeeper configured in the config/servX.properties is working properly

     To start 4 brokers on a node, go to the kafka install directory and run the following commands: "env JMX_PORT=10000  bin/kafka-server-start.sh config/serv1.properties" "env JMX_PORT=10001  bin/kafka-server-start.sh config/serv2.properties" "env JMX_PORT=10002  bin/kafka-server-start.sh config/serv3.properties" "env JMX_PORT=10003  bin/kafka-server-start.sh config/serv4.properties".

     To see if kafka brokers are registered in zookeeper, go to zookeeper install directory and run `bin/zkCli.sh` to start zookeeper client window and run `ls /brokers/ids`.

     Same with ZooKeeper, you may need to clean old data that's located in disks of kafka brokers. Just `rm -rf <all_data_path>` in all your kafka nodes and directories.

3. Spark setup

     All spark streaming related parameters can be defined in `conf/99-user_defined_properties.conf`.

     Param Name                     |  Param Meaning
     -------------------------------|------------------
     spark.executor.memory	    | available memory for Spark worker machines
     spark.serializer               | 
     spark.kryo.referenceTracking   | relevant to data encoding format
     spark.streaming.receiver.writeAheadLog.enable | whether to enable Write Ahead Log
     spark.streaming.blockQueueSize |size of streaming block queue

     Spark streaming can be deployed as YARN mode or standalone mode. For YARN mode, just set `hibench.spark.master` to `yarn-client`. For standalone mode, set it to `spark://spark_master_ip:port` and run `sbin/start-master.sh` in your spark home.

4. Storm setup

     The conf file is `conf/storm.yaml`. Basically we configure following params:

     Param Name                 | Param Meaning
     ---------------------------|---------------------------------------
     supervisor.slots.ports     | number of workers in one supervisor (we set 3 slots each supervisor)
     nimbus.childopts           | jvm size of nimbus
     supervisor.childopts       | jvm size of supervisor
     worker.childopts           | jvm size of worker
     topology.max.spout.pending | pending spout threads that can be tolerated
 
     Run `bin/storm nimbus` to start nimbus and `bin/storm ui` to setup storm ui
     Run `bin/storm supervisor` to start storm supervisors

5. HiBench setup 

     Same as [step.2 in previous section](#hibenchconf).

     Streaming workloads is defined in `conf/99-user_defined_properties.conf`, in `hibench.streamingbench.benchname`. You may set it to a value of following: `identity`, `sample`, `project`, `grep`, `wordcount`, `distinctcount` and `statistics`.

     Other parameters can be adjusted in `conf/01-default-streamingbench.conf`.
     
     Param Name                 | Param Meaning
     ---------------------------|---------------------------------------
     hibench.streamingbench.prepare.mode   | push / periodic mode
     hibench.streamingbench.prepare.push.records | records to send in push mode
     hibench.streamingbench.prepare.periodic.recordPerInterval | records to send per interval in periodic mode
     hibench.streamingbench.prepare.periodic.intervalSpan | interval in periodic mode
     hibench.streamingbench.prepare.periodic.totalRound   | total round in periodic mode
     hibench.streamingbench.zookeeper.host		  | zookeeper host:port of kafka cluster
     hibench.streamingbench.receiver_nodes     		  | number of nodes that will receive kafka input
     hibench.streamingbench.brokerList       | Kafka broker lists
     hibench.streamingbench.direct_mode	     | direct mode selection (Sparkstreaming only)
     hibench.streamingbench.storm.home	     | storm home
     hibench.streamingbench.kafka.home	     | kafka home
     hibench.streamingbench.storm.nimbus     | host name of storm-nimbus
     hibench.streamingbench.storm.nimbusAPIPort | port number of storm-nimbus
     hibench.streamingbench.storm.ackon		| ack mode on/off for storm

     Note: For SparkStreaming, receiver mode (Spark version >= 1.4). The first run will always fail. You'll need to wait a few more minutes, running `prepare/zkUtils.sh` to ensure the topic has be created. Then re-run the workload again. For Spark version == 1.3, it'll be OK.

5. View the report:
   
   Goto `<HiBench_Root>/report` to check for the final report:
      - `report/hibench.report`: Overall report about all workloads.
      - `report/<workload>/<language APIs>/bench.log`: Raw logs on client side.
      - `report/<workload>/<language APIs>/monitor.html`: System utilization monitor results.
      - `report/<workload>/<language APIs>/conf/<workload>.conf`: Generated environment variable configurations for this workload.
      - `report/<workload>/<language APIs>/conf/sparkbench/<workload>/sparkbench.conf`: Generated configuration for this workloads, which is used for mapping to environment variable.
      - `report/<workload>/<language APIs>/conf/sparkbench/<workload>/spark.conf`: Generated configuration for spark.
      
   Note, the throughput and lattency of each batch is printed to terminal during running streaming work endlessly. For SparkStreaming, press ctrl+c will stop the works. For Storm & Trident, you'll need to execute `storm/bin/stop.sh` to stop the works. For Samza, you'll have to kill all applications in YARN manually, or restart YARN.


---
### Advanced Configurations ###

1. Parallelism, memory, executor number tuning:

          hibench.default.map.parallelism       Mapper numbers in MR, 
                                                partition numbers in Spark
          hibench.default.shuffle.parallelism   Reducer numbers in MR, shuffle 
                                                partition numbers in Spark
          hibench.yarn.executors.num            Number executors in YARN mode
          hibench.yarn.executors.cores          Number executor cores in YARN mode 
          spark.executors.memory                Executor memory, standalone or YARN mode
          spark.driver.memory                   Driver memory, standalone or YARN mode

   Note: All `spark.*` properties will be passed to Spark runtime configuration.

2. Compress options:

          hibench.compress.profile              Compression option `enable` or `disable`
          hibench.compress.codec.profile        Compression codec, `snappy`, `lzo` or `default`
     
3. Data scale profile selection:

          hibench.scale.profile                 Data scale profile, `tiny`, `small`, `large`, `huge`, `gigantic`, `bigdata`
                                                  
   You can add more data scale profiles in `conf/10-data-scale-profile.conf`. And please don't change `conf/00-default-properties.conf` if you have no confidence.

4. Configure for each workload or each language API:

     1. All configurations will be loaded in a nested folder structure:

              conf/*.conf                                         Configure globally
              workloads/<workload>/conf/*.conf                    Configure for each workload
              workloads/<workload>/<language APIs>/.../*.conf     Configure for various languages

     2. For configurations in same folder, the loading sequence will be
     sorted according to configure file name. 

     3. Values in latter configure will override former.

     4. The final values for all properties will be stored in a single
     config file located at `report/<workload><language APIs>/conf/<workload>.conf`,
     which contain all values and pinpoint the source of the configures.

5. Configure for future Spark release

   By default, `bin/build-all.sh` will build HiBench for all running
   environments:

          - MR1, Spark1.2
          - MR1, Spark1.3
          - MR2, Spark1.2
          - MR2, Spark1.3
          - MR2, Spark1.4
          - MR2, Spark1.4

   And HiBench will probe Hadoop & Spark release version and choose proper HiBench release automatically. However, for furture Spark release (for example, Spark1.4) which is API compatibled with Spark1.3. HiBench'll fail due to lack the profile. You can define Hadoop/Spark release version by setting to force HiBench using Spark1.3 profile:

          hibench.spark.version          spark1.3

6. Configures for running workloads and language APIs:
  
  The `conf/benchmarks.lst` file under the package folder defines the
  workloads to run when you execute the `bin/run-all.sh` script under
  the package folder. Each line in the list file specifies one
  workload. You can use `#` at the beginning of each line to skip the
  corresponding bench if necessary.

  You can also run each workload separately. In general, there are 3
  different files under one workload folder.

      prepare/prepare.sh            Generate input data in HDFS for
                                    running the benchmark
      mapreduce/bin/run.sh          run MapReduce language API
      spark/java/bin/run.sh         run Spark/java language API
      spark/scala/bin/run.sh        run Spark/scala language API
      spark/python/bin/run.sh       run Spark/python language API


---
### Possible issues ###

1. Running with CDH/MR1:

   For a tarball deployed CDH/MR1, please recreate symlink file `hadoop-*-cdh*/share/hadoop/mapreduce` to point to correct folder:

          cd share/hadoop
          rm mapreduce
          ln -s mapreduce1 mapreduce

2. Running Spark/Python, MLLib related workloads:

   You'll need to install numpy (version > 1.4) in master & all slave nodes.

   For CentOS(6.2+):
     
     `yum install numpy`

   For Ubuntu/Debian:

     `aptitude install python-numpy`




