#bin directory of spark installation (will use spark-submit)
#SPARK_BIN_DIR=/home/lv/intel/cluster/spark/spark-1.3.0-bin-hadoop2.4/bin
####### comment: redirect to ${SPARK_HOME}/bin


###############

#Cluster config
# Spark master location
#sparkMaster=spark://lv-dev:7077
####### comment: redirect to SPARK_MASTER

# zookeeper host of kafka cluster
#zkHost=localhost
####### comment: add
hibench.streamingbench.zookeeper.host	localhost

###############

#Parallel config
# number of nodes that will receive kafka input
#receiverNodes=48
####### comment: add
hibench.streamingbench.receiver_nodes	48

###############
#Benchmark args
#Note to ensure benchName to be consistent with datagen type. Numeric data for statistics and text data for others
# please uncomment one benchName to run the benchmark
#benchName="micro/identity"
#benchName="micro/sample"
#benchName="micro/sketch"
#benchName="micro/grep"
#benchName="micro/wordcount"
#benchName="micro/distinctcount"
#benchName="micro/statistics"
####### comment: add
# available benchname: micro/identity micro/sample micro/sketch micro/grep micro/wordcount micro/distinctcount micro/statistics
hibench.streamingbench.benchname	micro/identity

#common args
# the topic that spark will receive input data
#topicName=identity-source-60
# commont: add
hibench.streamingbench.topic_name	identity-source-60

# Spark stream batch interval
#batchInterval=50  #In seconds
####### comment: add
hibench.streamingbench.batch_interval	50

# consumer group of the spark consumer for kafka
# consumerGroup=xxx
hibench.streamingbench.consumer_group	xxx

# expected number of records to be processed
#recordCount=900000000
hibench.streamingbench.record_count	900000000

#sketch/distinctcount/statistics arg
# the field index of the record that will be extracted
#fieldIndex=1
hibench.streamingbench.field_index	1

#sketch/wordcount/distinctcount/statistics arg
# the seperator between fields of a single record
#separator=\\s+
hibench.streamingbench.separator	\\s+

#sample arg
# probability that a record will be taken as a sample
#prob=0.1
hibench.streamingbench.prob		0.1

#grep arg
# the substring that will be checked to see if contained in a record
#pattern=the
hibench.streamingbench.pattern		the

#common arg
# indicate RDD storage level. 
# 1 for memory only 1 copy. Others for default mem_disk_ser 2 copies 
#copies=2
hibench.streamingbench.copies		2

# indicate whether to test the write ahead log new feature
# set true to test WAL feature
#testWAL=false
hibench.streamingbench.testWAL		false

# if testWAL is true, this path to store stream context in hdfs shall be specified. If false, it can be empty
#checkpointPath=
hibench.streamingbench.checkpoint_path	

#common arg
# indicate whether in debug mode for correctness verfication
#debug=false
hibench.streamingbench.debug		false

# whether to use direct approach or not
#directMode=true
hibench.streamingbench.direct_mode	true

# Kafka broker lists, used for direct mode, written in mode "host:port,host:port,..."
#brokerList=""
hibench.streamingbench.broker_list	""




# storm bench conf

#STORM_BIN_HOME=?
####### comment: add
hibench.streamingbench.storm.home	


###############

#Cluster config
# nimbus of storm cluster
#nimbus=
#nimbusAPIPort=6627
####### comment: add
hibench.streamingbench.storm.nimbus	
hibench.streamingbench.storm.nimbusAPIPort	6627

# time interval to contact nimbus to judge if finished
#nimbusContactInterval=
hibench.streamingbench.storm.nimbusContactInterval	


###############

#Parallel config
# number of workers of Storm. Number of most bolt threads is also equal to this param.
#workerCount=
hibench.streamingbench.storm.worker_count	

# number of kafka spout threads of Storm
#spoutThreads=
hibench.streamingbench.storm.spout_threads

# number of bolt threads altogether
#boltThreads=
hibench.streamingbench.storm.bolt_threads	

# kafka arg indicating whether to read data from kafka from the start or go on to read from last position
#readFromStart=false
hibench.streamingbench.storm.read_from_start	false

# whether to turn on ack
#ackon=true
hibench.streamingbench.storm.ackon		true

###############

#Benchmark args
#Note to ensure benchName to be consistent with datagen type. Numeric data for statistics and text data for others
# please  uncomment one benchName to run the benchmark
#benchName=micro-identity
#benchName=micro-sample
#benchName=micro-sketch
#benchName=micro-grep
#benchName=micro-wordcount
#benchName=micro-distinctcount
#benchName=micro-statisticssep
#common args
# the topic that storm will receive input data
#topic=
# target number of records to be processed
#recordCount=
####### comments: follow sparkstreaming's conf

# consumer group of the storm consumer for kafka
#consumer=
####### what's the differenct with sparkstreaming's consumerGroup?
hibench.streamingbench.storm.consumer	

#sketch/wordcount/distinctcount/statistics arg
# the seperator between fields of a single record 
#separator=\\s+

#sketch/distinctcount/statistics arg
# the field index of the record that will be extracted
#fieldIndex=1

#sample arg
# probability that a record will be taken as a sample 
#prob=0.1

#grep arg
# the substring that will be checked to see if contained in a record
#pattern=the



# added by Lv:
hibench.streamingbench.jars		${hibench.streamingbench.sparkbench.jar}
hibench.streamingbench.sparkbench.jar	${hibench.home}/src/streambench/sparkbench/target/streaming-bench-spark_0.1-4.0-SNAPSHOT.jar
hibench.streamingbench.stormbench.jar	${hibench.home}/src/streambench/stormbench/target/streaming-bench-storm-SNAPSHOT-jar-with-dependencies.jar


